\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan,}
\urlstyle{same}
\usepackage{multirow}

\title{LAnguage Models EnAble Simple Systems For Generating Structured VieWs of Heterogeneous DATA ते LAKES Siman Arora ${ }^{1}$, Brandon Yang ${ }^{1}$, Sabri Eyuboglu' , Avanika Narayan ${ }^{1}$, Andrew Hojel' ${ }^{1}$, Immanuel Trummer², and Christopher Ré ${ }^{1}$ 
 ${ }^{1}$ Stanford University 
 ${ }^{2}$ Cornell University }

\author{}
\date{}


\begin{document}
\maketitle
April 21, 2023

\section{AbStract}
$\checkmark$ A long standing goal of the data management community is to develop general, automated systems $\geqslant$ that ingest semi-structured documents and output queryable tables without human effort or domain on specific customization. Given the sheer variety of potential documents, state-of-the art systems make o simplifying assumptions and use domain specific training. In this work, we ask whether we can 7 maintain generality by using large language models (LLMs). LLMs, which are pretrained on broad data, can perform diverse downstream tasks simply conditioned on natural language task descriptions. - We propose and evaluate EVAPORATE, a simple, prototype system powered by LLMs. We identify $\checkmark$ two fundamentally different strategies for implementing this system: prompt the LLM to directly extract values from documents or prompt the LLM to synthesize code that performs the extraction. Our evaluations show a cost-quality tradeoff between these two approaches. Code synthesis is cheap, - but far less accurate than directly processing each document with the LLM. To improve quality while $>$ maintaining low cost, we propose an extended code synthesis implementation, EVAPORATE-CODE+, which achieves better quality than direct extraction. Our key insight is to generate many candidate functions and ensemble their extractions using weak supervision. EVAPORATE-CODE+ not only 중 outperforms the state-of-the art systems, but does so using a sublinear pass over the documents with the LLM. This equates to a $110 \times$ reduction in the number of tokens the LLM needs to process, averaged across 16 real-world evaluation settings of 10k documents each.

\section{Introduction}
Organizations often seek insights trapped in heterogeneous data lakes (e.g. the web, corporate data lakes, and electronic health records) $[8,25,52]$. In their raw form, these data sources cannot easily support analytical queries. A long standing goal of the data management community is to develop systems that automatically convert heterogeneous data lakes into queryable, structured tables $[10,13,44,47$, inter alia.]. In this work, we investigate whether recent large language models can help address this problem.

We study systems that take as input heterogeneous documents (e.g. HTML webpages, PDFs, text) and output a tabular, structured view of the documents. These systems must identify the schema and perform extraction to populate the table.

EXAMPLE 1. Medical researchers frequently use data spanning electronic health records (EHR), clinical trials, knowledge sources (e.g. PubMed), and FDA reports to understand and monitor patients and treatments [6]. As a motivating setting, we consider the large collection of FDA 510(k) reviews for premarket notification submissions for medical devices, which have been the subject of multiple studies [61, 64]. Our objective is to output a table that automatically structures the attributes that are distributed in the $\sim 20$-page PDFs (some example attributes are device classification, predicate device code, and indications for use). Input

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-02(2)}
\end{center}

EVAPORATE-CODE+

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-02(3)}
\end{center}

Output

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-02(1)}
\end{center}

Tables: A structured view of the data in the input documents.

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-02}
\end{center}

Figure 1: The user provides a collection of raw documents (e.g. NBA player Wikipedia pages) and EVAPORATE outputs a table by identifying attributes and populating columns. EVAPORATE avoids running expensive LLM inference on all documents by (1) synthesizing the key attributes from a small sample of documents and (2) synthesizing (e.g. Pythonic) functions that then are reused at scale to process documents. Because function quality is variable, EVAPORATE applies an algorithm that generates many candidate functions and ensembles their extractions using weak supervision.

Systems designed to tackle this problem must balance a three-way tradeoff between cost (data lakes may hold millions of documents), quality (output tables should be able to accurately support an analyst's queries), and generality (different data lakes have different document types and structure). See Section 2 for a formal task definition and further discussion of this tradeoff.

Given the sheer variety of formats, attributes, and domains across documents, prior systems rely on simplifying assumptions (e.g. focusing on one document format). A long line of work focuses on structuring HTML [10, 13, 23]. The state-of-the-art systems assume attributes and values are at specific positions in the HTML-DOM [21, 42, 43, 63]. For unstructured text, current approaches use linguistic tools (e.g., dependency parsers) to introduce structure [13, 23, 29, 48]. The documents in Example 1 highlight the limitations of the prior approaches: they lack grounding HTML structure and, consistent with recent evaluation efforts [63], we find the SoTA approaches for unstructured text perform poorly on long semi-structured PDFs (See Appendix C.1). To support new domains, one class of systems assumes access to a human-in-the-loop who can label data and write code [51, 55], while others assume access to annotated training documents from the domain [21, 42, 43]. Researchers manually annotated the reports in Example 1 [61].

In this work, we explore whether we can avoid simplifying assumptions and maintain generality by leveraging large language models (LLMs). An LLM is a deep learning model that is pretrained on broad data and can be adapted to diverse tasks, from machine translation to data wrangling $[12,46]$. At inference time, the models take as input a natural language task description termed a prompt $[9,12]$ and generate a natural language response. See Section 2.3 for more background on LLMs.

EVAPORATE. (Section 3) We propose and evaluate EVAPORATE, a simple system that uses LLMs to produce structured views of semi-structured data lakes. Our evaluation spans 16 distinct settings representing a range of real-world data lakes: from movie and university websites (e.g. IMDB) to FDA $510(k)$ reviews for medical devices [21, 30, 32, 36, 42, 61, 64].

EVAPORATE exposes a general interface that can be used across these varied settings: the user inputs any collection of raw documents and EVAPORATE automatically identifies the schema and extracts the attribute values to populate the table. Our implementation handles the diverse evaluation settings out-of-the-box, without any customization, training, or human effort. ${ }^{1}$ We propose two fundamental strategies for implementing this interface:

\section{Evaporate-Direct (Figure 2) The LLM directly extracts values from documents.}
\begin{enumerate}
  \setcounter{enumi}{1}
  \item Evaporate-Code (Figure 4) The LLM synthesizes code that is applied to process documents at scale.
\end{enumerate}

We evaluate the strategies and identify a fundamental tradeoff between cost and quality. Code synthesis is cheap, but far less accurate than directly processing each document with the LLM. EVAPORATE-CODE underperforms EVAPORATE-DIRECT by $24.9 \%$ (13.8 F1 points) averaged across our evaluation settings.

${ }^{1}$ Recent work applying prompting to data management requires customizing the prompt for each data setting [46, 57]. We next propose a more involved code synthesis implementation, EVAPORATE-CODE+, which achieves better quality than direct extraction. Our key insight is to generate many candidate functions and ensemble their extractions using weak supervision.

Direct Extraction (Section 3.1). Our first implementation, EVAPORATE-DIRECT, applies a single prompt (included in Appendix E) to each document in the input. The prompt instructs the LLM to both identify the schema and extract values. Remarkably, we find that in some settings, with a single prompt and no task specific modifications, performance is already competitive with state-of-the-art systems that rely on domain specific assumptions and training.

However, this implementation is very expensive. LLMs are optimized for interactive, human-in-the-loop applications (e.g. ChatGPT) [62], not high-throughput data processing tasks [54]. The number of tokens processed by an LLM in EVAPORATE-DIRECT grows linearly with the size of the data lake. As of March 2023, applying OpenAI's models to the 55 million Wikipedia articles would cost over \$110k (gpt-3.5, \$0.002/1k tokens) and \$1.1M (text-davinci-003, $\$ 0.02 / 1 \mathrm{k}$ tokens) dollars $[1,49]$. There are billions of webpages on the broader Internet [33]. Moreover, in most organizations, data processing is a routine expense repeated by multiple data analysts, not a one-time cost [53]. Data lakes are dynamically changing; new NBA players are added to Wikipedia over time, players' team attribute values change sporadically due to trades, and players' points per game change after every game. EVAPORATE-DIRECT would need to be repeatedly applied.

Code Synthesis (Section 3.2). Can we produce the structured table using a sublinear pass of the LLM over the documents? We propose EVAPORATE-CODE, which splits the task into two sub-tasks: (1) identify the table schema and (2) extract values. This view allows us to exploit the distinct redundancies of each sub-task that occur when running LLM inference on every document:

\begin{enumerate}
  \item Schema Generation. In order to identify a schema, we only process a small sample of documents with the LLM. This works because of redundancy in the attributes mentioned across documents. In Example 1, most reports mention a predicate device name.

  \item Function Synthesis. Instead of processing every document with the LLM and prompting it to directly extract values, we prompt it to synthesize (e.g. Pythonic) functions, that can then be applied at scale across the documents. This works because of redundancy in the formatting of attribute-value pairs. For instance, the predicate device name may consistently appear in the format "Predicate device name: $k$ ".

\end{enumerate}

The number of tokens processed by the LLM in EVAPORATE-CODE is fixed and does not grow with the size of the data lake (as illustrated in Figure 3), addressing the cost issues of EVAPORATE-DIRECT. However, the LLM synthesizes variable quality functions, leading to tables that are up to 14 points in Pair F1 score worse than those produced using EVAPORATE-DIRECT.

Code Synthesis + Aggregation. (Section 3.3) To improve quality while keeping costs low, we propose a third implementation, EVAPORATE-CODE+. Studying the synthesized functions, we observe some only work for a narrow slice of documents, while others exhibit syntactic and logical errors. To reduce variance, we synthesize many candidate functions and then estimate their quality and aggregate their extractions using weak supervision. This builds on recent work [3], which applies weak supervision to prompting.

Weak supervision (WS) is a statistical framework for modeling and combining noisy sources with varied coverages without any labeled data [51,59]. However, WS is typically applied over human-generated functions while our setting consists of machine-generated functions. This results in several issues when attempting to apply existing WS tools. (1) WS theoretically assumes all noisy sources are better than random performance (50\% accuracy), while $40 \%$ of our generated functions are below 25\% (Section 3.2). (2) WS attempts to deploy functions that achieve high quality on narrow slices of data (high precision), and allow the function to abstain on data external to the slice (low recall). While humans can express when functions should abstain, the machine-generated functions do not contain this logic. This makes it difficult to identify and exploit the high precision, low recall functions. To handle the open WS setting, we present a novel algorithm for ensembling the functions.

EVAPORATE-CODE+ achieves a pair F1 score of 67.5 on average across evaluation settings, a 12.1 point increase over EVAPORATE-DIRECT, using text-davinci-003. The prototype system demonstrates the possibility of LLM-based data management systems that achieve high quality at low cost, while maintaining generality. Our work makes the following contributions.

\begin{enumerate}
  \item We show that LLM-based systems can achieve high quality for structured view generation, while providing a more general interface than prior systems. EVAPORATE-CODE+ outperforms the state-of-the-art systems (see Section 4) — which utilize in-domain training and document specific heuristics (e.g. DOM tags) — by 3.2 F1 points (6\%) when generating tables from scratch, and $6.7 \mathrm{~F} 1$ points (10\%) on the extraction step, given a predefined schema.

  \item We explore two fundamentally different strategies for implementing EVAPORATE, direct extraction and code synthesis, showing that there is a cost-quality tradeoff between the approaches. EVAPORATECODE reduces the number of documents that need to be processed by the LLM by $110 \times$ at $10 \mathrm{k}$ documents per data lake. However, EVAPORATE-DIRECT achieves significantly higher quality.

  \item We propose EvAPORATE-CODE+, a weak supervision based algorithm to help us reliably use the synthesized functions. EVAPORATE-CODE+ outperforms EVAPORATE-DIRECT, which directly processes every document, by $12.1 \mathrm{~F} 1$ points (22\%) on average.

  \item We validate that the tradeoffs hold across multiple models. We evaluate EVAPORATE using four models from three unique LLM providers [4, 41, 49] and observe relative quality of EVAPORATE-DIRECT vs. EVAPORATE-CODE+ remains consistent as we vary the LLM.

\end{enumerate}

This paper is structured as follows. We define the problem in Section 2. We present EvAPORATE in Section 3, evaluations in Section 4, and discussion and related works in Sections 5 and 6. We release code at \href{https://github}{https://github}. com/HazyResearch/evaporate.

\section{Preliminaries}
We first define the problem setting and system desiderata.

\subsection{Problem Setting}
We study the problem of constructing a structured view (i.e. database table) of a set of semi-structured documents (e.g. HTML, PDF, TXT). Formally, we define the problem as follows:

\begin{itemize}
  \item Input: User provides a set of $n$ semi-structured documents $D=\left\{d_{1}, d_{2}, \ldots d_{n}\right\}$ (e.g. A collection of FDA $510(\mathrm{k})$ reviews for premarket notification submission for medical devices).

  \item Output: System outputs a table defined by a set of attribute names $A=\left\{a_{1}, a_{2}, \ldots a_{m}\right\}(e . g$. $a_{1}=$ indications for use, $a_{2}=$ classification) and a set of $n$ extracted records for $R=\left\{r_{1}, r_{2}, \ldots r_{n}\right\}$, one per document, where $r_{i}$ is an $m$-tuple (e.g. $r_{1}=$ ("fracture", "x-ray")).

\end{itemize}

Unlike prior work which proposes systems that rely on manual labeling [55] or manual prompt tuning [46], we aim to develop fully automated solutions, which require no additional user interaction beyond specifying the input documents.

How do we evaluate? We compare the generated table $(A, R)$ to a manually curated "ground-truth" table $(\hat{A}, \hat{R})$. The coverage of an attribute refers to the fraction of documents that include the attribute and its value. Following prior work, we prioritize attributes with high coverage, which tend to be useful for analysis [14, 17]. We measure agreement between the tables using Pair F1. For additional details on our evaluation setup, see Section 4.3.

\subsection{System Desiderata}
Current systems for producing structured views are limited in their generality, cost/flexibility, and quality/usability $[14,17,48,63]$. Here we review the existing systems.

Generality. The ideal system will generalize across document formats and domains, without manually engineered rules or task-specific training. This is important because the input documents $D$ could focus on any imaginable topic or use any file format [63]. Existing systems featurize documents by tagging the named entities (NER), dependency parse tree, and part-of-speech (POS), and train a model to predict whether a span of text is a useful fact [38]. Unfortunately, the performance of the parse, NER, and POS tags drastically degrade on semi-structured data (e.g. HTML elements) and longer sequences of text (i.e. full documents) [63]. We provide detailed error analysis in Appendix C.1. A specialized class of systems focuses on processing semi-structured web HTML documents by leveraging the HTML DOM tree as features $[11,14,21,23,43$, inter alia.]. However, the systems thus do not support other document formats.

Cost. The ideal system will enable users to manage a cost-coverage tradeoff, rather than requiring them to extract "all-or-nothing". The existing systems are built to extract all possible facts in the documents, without prioritizing important attributes or allowing the user to influence what is extracted [20,63]. Processing every line of every document can be expensive. To mitigate this, the user can define the attributes of interest then apply a closed IE system for

\section{Integrated Prompt}
\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-05}
\end{center}

Figure 2: Prompt for EVAPORATE-DIRECT structured. The prompt template, which includes placeholders for in-context examples and and the inference example (i.e., data lake documents in the context of our problem), is applied to each document in the data lake.

extraction, however this requires upfront human effort. Desiderata: The ideal system will enable users to manage a cost-coverage tradeoff, rather than requiring them extract "all-or-nothing".

Quality. The ideal system will output a table $(A, R)$ with full columns (i.e. high-coverage attributes) and accurate, consistently formatted extractions. Existing OpenIE systems commonly extract tuples in unnormalized forms directly from documents [20]. This can make the resulting extractions difficult to use for analysis, requiring advanced systems or user-defined post-processing code for resolving subject, objects, and predicates to a canonical form [13].

\subsection{Background on Large Language Models}
In this section, we provide background on large language models (LLMs), which are central to our work.

Definition 1 (LARge Language Model) A machine learning model, $\mathcal{F}$, trained on a self-supervised task (e.g. next word prediction) over a massive corpus of text [27]. Language models can be used to generate new text based on provided context. For example:

$$
\mathcal{F}(\text { All that glitters }) \rightarrow \text { is not gold. }
$$

Numerous studies have demonstrated LLMs capable of solving new tasks without updating any model parameters, a phenomenon termed in-context learning [2, 12,46]. Specifically, these studies show that when passed an appropriate description of the task, the model often generates text completing the task.

DEFINITION 2 (PROMPT) A natural language task-specification used to elicit a particular generation from an LLM. Prompts often include demonstrations of the task. For example, the prompt below elicits the translation of the word cheese into French:

$$
\mathcal{F}(\underbrace{\text { Translate. Eng: hello, Fr: bonjour; Eng: cheese, Fr: }}_{\text {Prompt }}) \rightarrow \underbrace{\text { fromage }}_{\text {Generation }}
$$

Examples of prompts used in this work are provided in Figures 2 and 4. All prompts used in the system are provided in Appendix E.

\section{Evaporate: A Prototype System Powered by Language Models}
In this section, we describe EVAPORATE, a prototype system that uses LLMs to materialize a structured view of a heterogeneous, semi-structured data lake. Interface. Compared to prior systems, which rely on manual labeling [55] or tuning prompts to a domain [46], EVAPORATE exposes a remarkably general interface: the user inputs raw documents of any format and the system automatically outputs a structured view of those documents, without any domain specific training or prompt customization. We propose two implementations of this interface which tradeoff cost and quality.

Implementation. There are two fundamentally different approaches to implementing this interface with LLMs. Either we feed every document to the LLM and prompt it to extract values directly (direct extraction), or we feed a small sample of documents to the LLM and prompt it to write code to do the extraction (code extraction). An example prompt for the former approach is shown in Figure 2 and for the latter in Figure 4.

In Section 3.1 and Section 3.2, we describe baseline implementations of these two strategies, EVAPORATE-DIRECT and EVAporate-Code. We find that these two implementations tradeoff cost and quality. Then, in Section 3.3, we propose a code extraction implementation that uses weak supervision to improve quality.

\subsection{EVAPORATE-Direct}
In this section, we describe a simple direct extraction implementation, EVAPORATE-DIRECT that applies a single prompt template to every document. This prompt template, which is included in Figure 2, instructs the LLM to both identify the schema and extract values (see Appendix E for the full prompt). It consists of a few in-context examples that are general, i.e. are not customized to a particular format, domain, or document.

Below we discuss how we (1) manage long documents that cannot fit in the LLM's context window, (2) process the LLM's textual outputs, (3) prioritize the most useful attributes according to principles described in prior work [14].

Managing long documents. The input to EVAPORATE is a file path to raw documents, which can be several pages long. For instance the Medical FDA reports in Example 1 are $\sim 20$ pages long. However, the underlying Transformer architecture of modern LLMs is limited to processing a fixed number of tokens (e.g. a few thousand tokens), referred to as the context window, during each inference call. EVAPORATE therefore splits the raw documents such that each piece is within the context window. Each chunk is inserted into the prompt in turn as shown in Figure 2.

Processing text outputs. Language models output open ended text so the last step is to convert this to a usable table. To facilitate this data transformation, we can specify formats in our prompt demonstrations to encourage the LLM to organize the output in a similar structure. For instance, the demonstration in Figure 2 specifies a list format with : <value(s)> per entry. EVAPORATE outputs in this format can be de-serialize into a table.

Prioritizing common attributes. The list of extracted attributes and values can contain the niche attributes for specific documents, whereas a common database design principle is to capture the high frequency attributes [13]. Therefore EVAPORATE takes the union of attributes outputted across documents and ranks by frequency to enable prioritizing head attributes.

Analysis. We analyze this direct extraction implementation, EVAPORATE-DIRECT, along the axes of our three desiderata. Results processing the documents with EVAPORATE-DIRECT are reported in Table 3 and are discussed in detail in Section 4.

Overall, the quality matches or exceeds the baseline systems (described in Section 4), on 8 of the 16 settings. This is surprising given the simplicity - i.e. EVAPORATE-DIRECT uses one fixed prompt to process all 16 settings. However, fundamental cost limitations impede the real-world deployment of this approach.

However, the high cost of this implementation limits its applicability to large, recurring workloads. The number of tokens processed by the LLM scales linearly with the size of the data lake, $\mathcal{O}(n)$. Data lakes can contain billions of documents $[33,47]$. Further, in most organizations, data processing is not a one time cost. Data lakes are dynamically changing, so EVAPORATE-DIRECT would need to be repeatedly applied.

\subsection{EVAPORATE-CODE}
In this section, we present EVAPORATE-CODE, which significantly reduces cost compared to EVAPORATE-DIRECT. Here, we perform schema identification separately from value extraction, which allows us to exploit fundamental differences between the sub-tasks to reduce cost. In schema identification, we find that we only need to process a small sample of documents because attributes are consistent across documents. On the other hand, in order to extract values, we must process every document. However, the ways in which values appear across documents (i.e. their relative positions in the document) tend to be consistent, meaning the extraction logic is consistent across documents.

The two steps of the decomposed implementation are:
\includegraphics[max width=\textwidth, center]{2023_08_17_bf24dbb59d8f7501c43fg-07}

Figure 3: Tradeoffs between processing the documents via direct prompting in EVAPORATE-DIRECT (Direct) versus code synthesis in EvAPORATE-CODE and EvAPORATE-CODE+ (Code). For small data lakes and large numbers of attributes, Direct is sufficient. As the number of documents grows, Code is orders-of-magnitude more efficient. Left is evaluated at 10 attributes, Right at 10K documents, assuming $10 \mathrm{~K}$ tokens per document.

\begin{enumerate}
  \item Schema synthesis. (Section 3.2.1) We observe that the attribute outputs contain relatively consistent , even though the values differ from document to document. To exploit this redundancy, EVAPORATE-CODE prompts an LLM to analyze a small sample of documents to identify attributes for the output schema. For example, given a sample of the Medical Device FDA Reports, the LLM outputs a devices table with attributes like "510(k) number".

  \item Function synthesis (Section 3.2.2). We observe consistencies in how attributes are embedded across documents. E.g., the $510(\mathrm{k})$ code in the FDA documents always starts with the letter "k" and the player position attribute is always in the HTML "infobox" element in NBA player Wiki pages. A researcher would likely exploit such redundancies when manually scraping the documents for analysis. In EVAPORATE-CODE, we propose to use the LLM to automatically synthesize a data-lake-specific suite of functions, that can then be applied at scale to process many documents.

\end{enumerate}

\subsubsection{Schema Synthesis}
EVAPORATE-CODE begins by identifying attributes $A=\left\{a_{1}, a_{2}, \ldots a_{m}\right\}$ for the output table's schema.

Generating candidate attributes Concretely, we sample a set $\tilde{D}$ of $k<<n$ documents from $D$. For each, we prompt the LLM to extract the most useful attributes from the document as in EVAPORATE-DIRECT. Recall this yields a set of attributes ranked by how frequently they were extracted across documents. We retain attributes that are explicitly mentioned in the document to ensure provenance in schema identification.

Re-ranking candidate attributes Because EVAPORATE-CODE now identifies the attributes from a small set of documents, we observe that EVAPORATE-CODE's ranking is noisier than when every document was processed in EVAPORATEDIRECT, i.e. an important attribute may be selected by the LLM a few times amongst the $k$ documents. To address this, we introduce a new prompting step in which we include the union of extracted attributes and instruct the LLM to identify the most useful attributes (see Appendix E for the prompt). High quality on this task reflects the powerful reasoning capabilities of recent LLMs. Finally, the frequency-based rank is upweighted by a constant multiplicative factor if the attribute is included in the LLM output.

\subsubsection{Function Synthesis}
Given the attributes $A=\left\{a_{1}, a_{2} \ldots a_{m}\right\}$, the objective of EVAPORATE-CODE's second phase is to extract the values of the attributes for each document $d_{i} \in D$. Our key insight, as discussed, is that attribute-values are expressed in similar ways from document to document. To exploit this, instead of processing every document with the LLM to extract values for attribute $a_{i}$, we propose to use the LLM to generate code that can then be reused to process many documents.

Figure 4 shows an EVAPORATE-CODE function synthesis prompt. The in-context examples show pairs of text snippets and functions to extract an attribute of interest. EVAPORATE-CODE searches the data lake via a simple keyword search for document portions that mention $a_{i}$, and includes this in the prompt. EVAPORATE-CODE synthesizes functions for attributes following the rank-order of attributes derived during schema synthesis. This means that values of the most relevant (and frequent) attributes as determined by EVAPORATE-CODE are extracted first. The user can stop the synthesis when desired. Analysis. We briefly analyze the EVAPORATE-CODE implementation along the axes of our three desiderata. Results processing the documents with EVAPORATE-CODE are reported in Table 3 and are discussed in detail in Section 4.

Cost. The number of tokens processed by the LLM in EVAPORATE-CODE is fixed with respect to the number of documents. Figure 3 demonstrates the asymptotic differences in cost between EVAPORATE-DIRECT and EVAPORATE-CODE. As shown in the left plot, Evaporate is asymptotically more efficient as a function of the number of documents. This is because the number of LLM calls required with function generation is proportional to the number of attributes to be extracted, not the number of documents. The crossover point is at $\sim 40$ documents. In the right plot, we show EVAPORATEDIRECT has the potential to extract multiple attributes from the in-context document per inference call, while EVAPORATE-CODE requires generating new functions for each attribute. As a result, the cost of EVAPORATECODE grows with the number of attributes, while the cost of EVAPORATE-DIRECT approach is constant. The crossover point is at $\sim 2,500$ attributes.

Quality. The tables generated by EvAPORATE-CODE are on average 13.8 pair $\mathrm{F} 1$ points worse than those produced using Evaporate-DiRECT. This suggests that there is a cost-quality tradeoff between the two implementations, since EVAPORATE-CODE is much cheaper.

\subsection{EVAporate-Code +}
In this section we discuss an extension of EVAPORATECODE, which enables significant quality improvements while keeping costs low. This implementation, which we call EVAPORATE-CODE+, synthesizes many candidate functions and ensembles their extractions using weak supervision. We decompose the task into three parts:

\begin{enumerate}
  \item Schema identification. (Section 3.2.1) Same as in EVAPORATE-CODE.

  \item Function synthesis. (Section 3.3.1) Same as in EVAPORATE-CODE, except instead of generating a single function per attribute, we generate many candidate functions. Below we describe techniques to encourage diversity among candidates.

  \item Function aggregation. (Section 3.3.2) The synthesized candidate functions have varying qualities and coverages, making them unreliable. We then introduce a weak supervision (WS) based algorithm to aggregate over their different predictions for the attribute values across documents.

\end{enumerate}

\subsubsection{Synthesizing Diverse Candidate Functions}
We find that the quality of LLM-generated functions varies significantly depending on the document chunk and in-context examples used in the prompts. To address the variability in function quality, we adopt the strategy we previously proposed in Arora et al. [3]. This strategy curates multiple diverse prompt templates for the same task (i.e.

\section{Function Generation Prompt}
\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-08(1)}
\end{center}

Question: Write a python function called "get\_dates\_modified\_field" to extract the "DATES MODIFIED" field from the text. Include any imports.

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-08}
\end{center}

Question: Write a python function called "get\_date\_published\_field" to extract the "datePublished" field from the text. Include any imports.
\includegraphics[max width=\textwidth, center]{2023_08_17_bf24dbb59d8f7501c43fg-08(2)}
two data lake agnostic in-context examples.
Figure 4: A representative prompt for function synthesis with multiple function generation prompts in the style of Figure 4) and prompts the LLM with each in turn to produce a diverse set of function candidates $F=\left\{f_{1}, f_{2}, \ldots f_{k}\right\}$.

Concretely, we curate two function generation prompts $P_{A}$ and $P_{B}$ (both included in Appendix E) $P_{A}$ includes no in-context examples, only a task description that encourages the LLM to use regular expressions. $P_{B}$ includes two in-context examples along with the task description that encourages the LLM to import and use any Python library of choice. We find that neither consistently outperforms the other. $P_{A}$ produces higher quality functions on $69 \%, 45 \%$, $60 \%, 91 \%$, and $31 \%$ of attributes on the 8 SWDE Movie, 5 SWDE University, FDA reports, Enron, and Wikipedia player pages settings respectively. Writing a single "perfect" prompt that performs well across all documents can be challenging and prompting the LLM to complete the task in multiple ways helps.

\subsubsection{Aggregating Candidate Functions}
Next, we discuss how to combine the aggregations of the candidate functions.

Background: Methods for Unsupervised Aggregation Because we lack ground truth labels in our setting, it is not possible to directly evaluate the quality of the candidate functions. A popular unsupervised aggregation strategy is to take the Majority Vote (MV) across function outputs [60]. Formally, MV treats the functions as independent of one another and assigns equal weight to all function outputs. However, the functions are not of equal quality - over $40 \%$ of synthesized functions result in less than 25 Text F1 in extraction quality. Therefore, EVAPORATE uses weak supervision (WS), a popular standard statistical framework for modeling the accuracies and correlations between noisy sources of information without any labeled data [26, 51]. WS is widely used in industry [51].

Unfortunately, the standard programmatic WS setup makes several assumptions that do not apply in our setting. The gap arises because the standard programmatic WS setup assumes human-designed functions that output standardized classes for classification. In contrast, the functions in our setting are machine-generated functions that output non-standardized extracted text. Our setting violates the following assumptions of the existing WS setting:

\begin{enumerate}
  \item Assumption 1: Functions will abstain on examples where they do not apply [26, 51]. WS is able to exploit high-precision functions that may not have high recall. When humans construct the functions, they typically specify conditions under which the function is applicable (e.g. "If the email has a URL, "vote" that it contains spam, otherwise abstain" [56]). However, it can be challenging to generate functions with this advanced logic. As a result, the machine-generated functions from our prompts always provide some output. This is particularly important when the function is not able to extract a value from a document, which can either be because the attribute does not exist in the document ("No Attribute Value", e.g. a Wikipedia page may be missing a college attribute since historically, not all basketball players have attended college) or because the attribute exists but the function does not apply to this specific instance ("Function Abstension", e.g. the product code attribute value starts with a lowercase " $\mathrm{k}$ " in the minority of FDA reports and capital " $\mathrm{K}$ " in the majority. EVAPORATE-CODE generates functions for both cases, which output the product code on relevant documents, and empty strings otherwise.). If we cannot distinguish between cases, we will not be able to exploit high precision / low recall functions in our setting.

  \item Assumption 2: Functions are correlated with the ground truth label $y$ at better than random performance [26, 51]. While this is reasonable when functions are human-designed, EVAPORATE uses machinegenerated functions. We find $51 \%$ of generated functions yield $<50$ Text $\mathrm{F} 1$.

  \item Assumption 3: Weak supervision is typically applied to tasks with well defined classes in a classification setting [26, 51]. In our case, the output of the functions are extracted text, and thus there is a virtually unconstrained output space of possible extractions that vary from document to document (e.g. NBA players have varied date of birth values). The number of unique extractions collected by the functions can also differ across documents.

\end{enumerate}

We propose the following approach to be able to leverage WS. Let $D_{\text {eval }}$ be a small sample of documents from the data lake $\mathcal{D}$. We have the set of generated functions $F$ and LLM $\mathcal{F}$.

Identifying function abstentions. Our objective is to estimate the probability that an empty output from a function is an abstension. We propose to measure the fraction $e$ of the $D_{\text {eval }}$ documents for which $\mathcal{F}$ extracts a value. Intuitively, when $e$ is high, our prior should be that the attribute appears in a large fraction of documents, so we should assume functions are abstaining when they output empty values. When $e$ is low, the attribute appears in few documents, so we should assume the functions are predicting empty values. We can use $e$ to guide both our function evaluation and downstream aggregation. Note that it is possible for $\mathcal{F}$ to abstain or hallucinate values when they do not exist in the document, affecting the estimate of $e$.

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-10}
\end{center}

Filtering functions with worse than random performance. One key observation is that we can utilize the high quality LLM $\mathcal{F}$ on a small set of documents $D_{\text {eval }}$ (e.g. we use $\left|D_{\text {eval }}\right| \leq 10$ ). In Table 7, we validate that LLMs can generate high quality extractions for a wide range of attributes and document formats. We can leverage these as a high quality estimate of the ground truth extractions for those documents. We compute a score $s_{j}$ by comparing the outputs of function $f_{j}(\cdot)$ against the outputs of $\mathcal{F}$ on document $d_{i} \in \mathcal{D}_{\text {eval }}$. If we are in the low $e$ regime, we should evaluate the outputs on all $d \in \mathcal{D}_{\text {eval }}$. If we are in the high $e$ regime, we should evaluate the outputs on only the $d \in \mathcal{D}_{\text {eval }}$ for which the function extracted a value. We finally filter any function $f_{j}$ with scores $s_{j} \leq 0.5$, where 0.5 derives from the typical WS assumptions [26, 51, 58].

Handling extractions with unconstrained output spaces. The $k$ generated functions can produce $[0 . . k]$ unique prediction votes for a single unlabeled document $d_{i}$, and the number of unique votes can differ from document $d_{i}$ to $d_{j}$. Therefore, for each $d_{i} \in \mathcal{D}$, we bucket the unique votes and take the $b$ buckets representing the most frequently occurring votes. The votes for functions that outputted values outside the top- $b$ are marked as abstensions. If the number of unique votes is $<b$, placeholder values are inserted into the top- $b$. Finally, as the "classes" differ across documents, we introduce a constraint to the objective function encouraging the class-conditional accuracies to be equal.

After addressing these assumptions, we can leverage prior approaches to aggregate the noisy extractions from the function candidates into higher-quality extractions as in [3, 51]. Under WS, the output of each function is viewed as a "vote" for the true label and the objective is to construct a latent graphical model to account for the varied accuracies and correlations amongst the functions, without access to any labeled data. Our aggregation method is summarized in Algorithm 1.

Analysis. We briefly analyze the EVAPORATE-CODE+ implementation along the axes of our three desiderata. Results processing the documents with EVAPORATE-CODE+ are reported in Table 3 and are discussed in detail in Section 4.

Cost. As with EVAPORATE-CODE, the number of tokens processed by the LLM in EVAPORATE-CODE+ is fixed with respect to the number of documents. Figure 3 demonstrates the asymptotic differences in cost between EVAPORATEDIRECT and EVAPORATE-CODE. The number of tokens that must be processed by the LLM grows only by a constant factor: the number of function candidates generated. The user can set this number to balance cost and quality.

Quality. Of the three implementations, EVAPORATE-CODE+ produces the highest quality tables. EVAPORATE-CODE+ outperforms EVAPORATE-DIRECT by $12.1 \mathrm{~F} 1$ points (22\%) on average, while using far fewer computational resources. Using function aggregation leads to an improvement of $25.1 \mathrm{~F} 1$ points over EVAPORATE-CODE.

\section{Evaluations}
We evaluate the three EVAPORATE system implementations across 16 document formats spanning five domains representing a range of real-world settings. Our evaluation is designed to validate the following claims:

\begin{itemize}
  \item Function synthesis enables asymptotic cost reductions for processing data with LLMs. There has been significant recent interest in developing various data management applications with LLMs $[16,35,39,46]$. Prior work directly processes data with the LLM. By synthesizing functions that process the data, EVAPORATECODE+ reduces the number of tokens the LLM needs to process by 110x relative to EVAPORATE-DIRECT.

  \item Function synthesis + aggregation results in higher quality than direct extraction. Despite the fact that EVAPORATE-DIRECT processes each document with the LLM directly, EVAPORATE-CODE+ performs 12.1 F1 points (22\%) better on average. Based on comparisons with EVAPORATE-CODE, which only synthesizes one function, we show that function aggregation is key in enabling the improvements.

  \item EVAPORATE achieves higher quality than state-of-the-art baselines, while exposing a more general interface. EVAPORATE-CODE+ expresses tasks via merely six natural language prompts (all provided in Appendix E) and uses no training. Yet, it exceeds SoTA systems by 3.2 F1 (6\%) points when generating tables from scratch and 6.7 points $(10 \%)$ when extracting pre-defined gold attributes. Meanwhile, it supports a broader range of settings than any of these baselines.

  \item The identified tradeoffs hold across language models. We evaluate on four models from three unique providers $[4,41,49]$. We find that the tradeoff space we identify between EVAPORATE-DIRECT and EVAPORATE-CODE+ holds across LLMs. The implementations remain competitive in quality across LLMs.

\end{itemize}

\subsection{Experimental Setup}
We primarily evaluate EVAPORATE on the end-to-end task of structured view generation. For the purpose of comparison to prior work, we also evaluate on the sub-task of closed information extraction. We first define these tasks, their metrics, and the baselines. We then provide implementation details for EVAPORATE.

Structured view generation task. This captures the end-to-end task of identifying the schema and populating the output table. This task is often discussed as a vision system [14], and given the difficulty of this task, there are limited comparable works. We therefore compare to the closest line of work, OpenIE systems, where the task is to extract all facts from documents [5, 48]. We compare to two sets of baselines: (1) Deng et al. [21], Lockard et al. [42, 43] for HTML-specific OpenIE, and (2) Kolluru et al. [38] for generic unstructured text. The former models explicitly use the HTML-DOM tree structure to process the page, assuming attribute values are leaf nodes, and explicitly train on documents from the domain of interest. The latter class of systems first label sentences using linguistic tools (i.e. dependency parsers, part of speech taggers, and named entity taggers), and fine tune LLMs over these features to perform the task [63].

Metrics. Following the baseline work [21], we report performance using Pair F1. This is an F1 score applied to the predicted vs. gold sets of tuples of the form (document ID $d_{i}$, attribute $a_{j}$, value $r_{i, j}$ ). All three elements in the tuple must exactly match a tuple in the ground truth to be marked correct. Since EVAPORATE ranks the attributes and generates functions in this order, for fair comparison, we report OpenIE scores for all tuples up to $k$ attributes, where $k$ is the number of gold attributes for the setting. We note that the prior systems extract all-or-no tuples, in contrast.

Closed information extraction task. This captures the setting where the user provides a pre-defined schema and EVAPORATE is used to populate the table. We compare to state-of-the-art approaches for ClosedIE including: (1) Deng et al. [21], Lockard et al. [42, 43] for HTML-specific ClosedIE and (2) Clark et al. [18], He et al. [31] for generic unstructured text. The former models explicitly use the HTML-DOM tree structure to process the page, assuming attribute values are leaf nodes, and explicitly train on documents from the test domain. The latter are pretrained LLMs that have been fine tuned on massive amounts of labeled (attribute, value) pairs [50]. We report ClosedIE results using the Text F1 metric on a value-by-value basis across each document.

EVAPORATE implementation details. In the following experiments, we instantiate EVAPORATE with currently popular, LLM APIs. Experiments in Sections 4.3 and 4.4.1 use text-davinci-003 from OpenAI. In Section 4.4.2, we evaluate additional LLMs from three model providers. For experiments, we use 10 sample documents per data lake for the schema synthesis, function synthesis, and function verification. We apply Algorithm 1 over the top-10 scoring functions that are synthesized for each attribute and data lake. The prompts remain constant across data lakes and models.

When the measuring cost for alternate implementations of EVAPORATE, we compute total number of tokens processed by the LLM to perform the end-to-end task (i.e. the sum of the number of tokens in the prompt and the number of tokens the model generates). We use this metric because the wall-clock time and dollar cost of a model fluctuate depending on the setup, but both should be proportional to the number of tokens processed.

\begin{center}
\begin{tabular}{lc|ccc}
\hline
\multirow{2}{*}{Source (Format)} & CLOSEDIE & \multicolumn{3}{c}{OPENIE} \\
 & $F 1$ & $R$ & $P$ & $F 1$ \\
\hline
FDA (TXT) & 80.1 & 58.9 & 67.2 & 62.8 \\
Enron Emails (TXT) & 93.3 & 80.3 & 94.6 & 86.9 \\
Wiki NBA (HTML) & 84.7 & 55.7 & 88.2 & 68.2 \\
SWDE Movie (HTML) & 79.5 & 48.5 & 71.0 & 56.8 \\
SWDE University (HTML) & 73.7 & 50.9 & 71.4 & 59.0 \\
\hline
Average & $\mathbf{8 2 . 3}$ & $\mathbf{5 8 . 9}$ & $\mathbf{7 8 . 5}$ & $\mathbf{6 6 . 7}$ \\
\hline
\end{tabular}
\end{center}

Table 1: Quality of EvAPORATE-CODE+ evaluated on ClosedIE in Text F1 and OpenIE in Pair F1 across all documents using text-davinci-003.

\begin{center}
\begin{tabular}{lcc|cc}
\hline
\multirow{2}{*}{System} & \multicolumn{2}{c}{SWDE MoviE} & \multicolumn{2}{c}{SWDE University} \\
 & Closed & Open & Closed & Open \\
\hline
ZeroShot Ceres [43] & - & 50.0 & - & 50.0 \\
RoBERTa-Base & 49.3 & 35.6 & 36.6 & 38.0 \\
RoBERTa-Structural & 47.7 & 39.9 & 46.5 & 42.3 \\
DOM-LM [21] & 71.9 & 54.1 & 68.0 & 55.2 \\
\hline
EVAPORATE-DIRECT & $\mathbf{8 4 . 4}$ & 37.4 & 72.6 & 54.4 \\
EVAPORATE-CODE & 55.0 & 33.0 & 40.5 & 22.2 \\
EVAPORATE-CODE+ & 79.5 & $\mathbf{5 6 . 8}$ & $\mathbf{7 3 . 7}$ & $\mathbf{5 9 . 0}$ \\
\hline
\end{tabular}
\end{center}

Table 2: Comparisons to state-of-the-art on ClosedIE in Text F1 and OpenIE in Pair F1. The baselines train on in-domain documents, while EVAPORATE uses no training. Baselines are as reported in [21] as code was not released.

\subsection{Evaluation Settings}
We evaluate EVAPORATE on 16 settings representing a range of real-world data lakes. First, we use a benchmark suite of 13 Movie and University websites to compare EVAPORATE to state-of-the-art information extraction systems [21, 30, 42]. Next, to evaluate on more unstructured data (i.e. non-HTML) we turn to: Enron a corporate email corpus that has been analyzed in over three thousand academic papers [32, 36], FDA 510(k) reviews for premarket notification submissions for medical devices, which have been the subject of multiple important research studies [61, 64], and NBA Wikipedia pages for NBA players, which include more complex HTML than the existing benchmarks [21]. We release the benchmarks and provide additional details in Appendix B. Here we briefly describe the properties we aim to study with each setting:

\begin{enumerate}
  \item Benchmark Suite: SWDE Movies \& Universities SWDE is the standard benchmark for document-level IE in prior work [21, 30, 42, 43, inter alia.]. There are 8 sets of webpages for Movies (e.g. IMDB, Rottentomatoes) and 5 sets of webpages for Universities (e.g. US News). For each website, the benchmark contains 1063-2000 pages and annotations for 8-274 attributes. We use this benchmark to effectively compare to the state-of-the-art and test on a range of attribute types, e.g. simpler Movie runtime through complex Movie cast and popular Movie director through infrequent second assistant director.

  \item Complex HTML: NBA The SWDE webpages attribute values are isolated in separate leaf nodes in the HTML-DOM tree. We use NBA Player Wikipedia pages evaluate on more complex HTML. For instance, the NBA draft attribute contains the draft round, year, pick number, and team by which the player was selected. We evaluate on 100 randomly selected player pages (spanning the 1940s-present) and 19 attribute annotations.

  \item Unstructured Text: Enron and FDA We observe a lack of existing benchmarks for document-level IE over unstructured text - intuitively, this setting has been challenging with prior generations of models due to the lack of any grounding structure whatsoever (i.e. recall current systems rely on HTML-DOM elements or sentence-level NER, dependency, and POS tags). We turn to the Enron and FDA settings described above. The Enron setting contains 15 gold attributes and 500k documents. The FDA setting contains 16 gold attributes and 100 PDF documents, which are up to 20 pages long, randomly sampled from FDA 510(k).

\end{enumerate}

\subsection{Comparing Evaporate to State-of-the-Art Baselines}
First we validate that EVAPORATE-CODE+ outperforms the state-of-the-art, both in terms of quality metrics, and in terms of the number of unique document formats and domains handled by a single system without any data lake specific customization. These baselines are as defined in Section 4.1.

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{3}{*}{Source} & \multicolumn{3}{|c|}{$\begin{array}{l}\text { EVAPORATE-DIRECT }\end{array}$} & \multicolumn{3}{|c|}{EVAPORATE-CODE+} & \multicolumn{2}{|c|}{Relative Performance} \\
\hline
 & \multirow{2}{*}{$\begin{array}{l}\text { Quality } \\
\text { F1 }\end{array}$} & \multicolumn{2}{|c|}{Cost / 10K Documents} & \multirow{2}{*}{$\begin{array}{l}\text { Quality } \\
\text { F1 }\end{array}$} & \multicolumn{2}{|c|}{Cost / 10K Documents} & \multirow{2}{*}{Quality} & \multirow{2}{*}{Cost Reduction} \\
\hline
 &  & Tokens (M) & Cost $(\$)$ &  & Tokens (M) & Cost $(\$)$ &  &  \\
\hline
FDA & 48.6 & 145.6 & 2,900 & 64.9 & 1.9 & 38 & +16.3 & $77 \mathrm{x}$ \\
\hline
Enron Emails & 90.9 & 21.2 & 425 & 87.1 & 0.6 & 12 & -3.8 & $35 x$ \\
\hline
Wiki NBA & 45.9 & 650.1 & 13,000 & 68.6 & 3.0 & 60 & +22.7 & $217 x$ \\
\hline
SWDE Movie & 37.4 & 282.9 & 5,660 & 57.4 & 2.3 & 46 & +38.0 & $123 x$ \\
\hline
SWDE University & 54.4 & 190.1 & 3,800 & 59.5 & 1.9 & 38 & +5.1 & $100 x$ \\
\hline
Average & $\mathbf{5 5 . 4}$ & 258 & $\mathbf{5 , 1 5 7}$ & 67.5 & 1.9 & 39 & +12.1 & $110 x$ \\
\hline
\end{tabular}
\end{center}

Table 3: Quality (OpenIE Pair F1) and cost (number of tokens processed by the LLM) for producing the structured views. We compare the direct prompting and code synthesis implementations using text-davinci-003. We evaluate quality on 10 randomly sampled documents due to the cost of EVAPORATE-DIRECT and here, report on the same sample for EVAPORATE-CODE+.

Overall, EVAPORATE performs the end-to-end task on documents spanning 16 formats and five domains. Averaged across settings, EvAPORATE-CODE+ provides 82.3 Text F1 on ClosedIE and 66.7 Pair F1 on OpenIE (Table 1). In Appendix F, we provide Figures showing samples of the documents that are input to EVAPORATE and the outputted tables.

Systems for semi-structured text Shown in Table 2, EVAPORATE-CODE+ outperforms the state-of-the-art on the canonical SWDE benchmarks. In contrast to EVAPORATE, which uses no training whatsoever, the baselines are limited to HTML documents and explicitly perform supervised learning using labels from webpages within the Movie and University domains respectively $[21,43]$

Critically, the baseline systems restrict their scope to attributes that are specifically mentioned in the HTML $<$ body $>$ text, even though attributes are frequently mentioned in the HTML header (e.g. within $\langle$ title $\rangle$ elements) as well as HTML tags within the body (e.g. ). Critically, EVAPORATE can identify and extract attributes mentioned anywhere in the document. To evaluate this, we extend the SWDE benchmark to include the attributes scattered throughout the full HTML and find EVAPORATE-CODE+ achieves 52.2 and 49.0 on Movies and University respectively on the more challenging setting. We release the new annotations.

Relatedly, the baseline systems assume attribute values are the leaf-nodes of the HTML-DOM tree and therefore are not applicable to the unstructured settings.

Systems for unstructured text We consider the state-of-the-art OpenIE6 system for performing OpenIE over unstructured (non-HTML) text from Kolluru et al. [38]. While this system is not designed to extract structured views from heterogeneous data, we evaluate it qualitatively to understand how existing OpenIE systems perform in this setting. We find the system only handles well formatted sentences and struggles to extend to heterogeneous data types. We find that even when documents contain full sentences, the system extracts an extremely large set of relations or enforce consistent extractions across documents. For instance, on a sample FDA 510(k) document, OpenIE6 extracts 427 relations with 184 relations having a confidence level at 0.99. We include a detailed error analysis in Appendix C.1.

\subsection{Comparing Implementations of EVAPORATE}
This work proposes a fundamental tradeoff space between directly processing data workloads with LLMs vs synthesizing code that does the processing. We first discuss the tradeoffs for a fixed LLM (text-davinci-003), which is the current best-in-class LLM [40] (Section 4.4.1), and next across a range of LLMs trained by three distinct model providers (Section 4.4.2).

\subsubsection{Tradeoffs between EVAPORATE Implementations}
As detailed in Section 3.2, the base routine ("EVAPORATE-DIRECT") in EVAPORATE entails directly processing documents with the LLM, while the optimized routine ("EVAPORATE-CODE") synthesizes functions for processing. Next we evaluate these along our desiderata.

Generality is maintained. LLMs take text as input and provide text as output - this unified natural language interface means EVAPORATE-DIRECT and EVAPORATE-CODE can ingest any document format without additional engineering. Critically, our results with EVAPORATE require no user effort, no training whatsoever, and no customization when applied to the 16 different settings.

Asymptotic cost reduction. Figure 3 demonstrates the asymptotic differences in cost between directly processing the data lake with Evaporate-Direct vs. with Evaporate-Code+. (Figure 3 Left) Evaporate-Code+ is asymptotically more efficient as a function of the number of documents in the data lake. The number of LLM calls required with function generation is proportional to the number of attributes to be extracted, not the number of documents. The crossover point is at $\sim 40$ documents.

(Figure 3 Right) EVAPORATE-Direct can extract multiple (i.e. every) attribute in the in-context documents in a single inference call, while EVAPORATE-CODE+ synthesizes new functions for each attribute. Thus, the cost of function synthesis grows with the number of attributes, while the cost of EVAPORATE-DIRECT is constant. The crossover point is at $\sim 2,500$ attributes.

Empirically across our settings, EVAPORATE-CODE+ realizes a 110x average reduction in the number of tokens the LLM needs to process (assuming 10k documents per setting and $378 \times$ given the true benchmark sizes) in the number of tokens the LLM must process compared to EVAPORATE-DIRECT (Table 3). Further, data lakes are constantly changing and functions can be reused while EVAPORATE-DIRECT would need to be re-run, multiplying the cost.

In runtime, we observe that the generated functions are efficient in processing the documents. For example, over the 9,500 function runs (from 95 functions evaluated on 100 documents each) in the FDA 510(k) setting, we find that the average time to run one function over one document is $0.00025 \mathrm{~s}$ on a 2 CPU machine.

Improved quality and reliability. Even though EVAPORATE-DIRECT directly processes each document with the LLM, EVAPORATE-CODE+ surprisingly performs 12.1 F1 (22\%) better (Table 3).

What are the failure modes of EVAPORATE-DIRECT? We profile the errors of EVAPORATE-DIRECT and find the main issue is inconsistency or a lack of reliability. On the Medical FDA report setting:(1) The LLM misses an average of 4.4 attributes that are present in the gold schema (27.5\% of gold attributes) per document. Among the gold attributes that are missed, all are extracted in at least one document. (2) Further, the LLM outputs an average of 9.7 attributes or values that are not explicitly mentioned in the documents. (3) Finally, attributes are reworded in diverse ways across documents - the attribute classification is extracted in 4 different ways across the sample of 10 documents (i.e. "classification", "device classification", "regulatory information", missing). As the error modes are quite varied, it is unclear how to improve quality. ${ }^{2}$

Why does EVAPORATE-CODE+ improve quality? We validate that our Algorithm 1 for selecting and aggregating functions leads to the quality improvements over EVAPORATE-DIRECT.

Synthesizing diverse functions We find that using diverse prompts helps address the lack of reliability in function synthesis. EVAPORATE-CODE+ applies a boilerplate prompt template to synthesize functions that contains no data lake specific examples or customization. Recall the prompt template includes one-to-two in-context demonstrations and a placeholder for the inference example, i.e. document text (Figure 4). Holding the inference example constant, it is trivial to instantiate multiple prompts in our provided template by simply swapping the in-context examples. We explore the effects of increasing diversity through each, and find both benefit downstream quality:

\begin{itemize}
  \item In-context demonstrations Our implementation (Table 1) instantiates two prompts by swapping in-context demonstrations, $P_{A}$ and $P_{B}$. Quality using $P_{A}$ or $P_{B}$ alone is 8.5 and $8.0 \mathrm{~F} 1$ points worse than using both to synthesize functions on SWDE Movie and SWDE University respectively.

  \item Inference documents Using five instead of three sample documents in the prompts for EVAPORATE-CODE+, the ClosedIE and OpenIE quality improve by $6.8 \mathrm{~F} 1$ points $(9 \%)$ and $6.5 \mathrm{~F} 1$ points $(14 \%)$ respectively, averaged across the 16 settings.

\end{itemize}

Estimating function quality using the LLM. In Table 4, we first evaluate the two unsupervised aggregation baselines in prior work off-the-shelf: Majority Vote (MV) and Weak Supervision (WS) [3, 51, 60]. Next we measure the effect of filtering functions and handling abstentions as proposed in Algorithm 1.

In Table 4, we observe WS with filtering provides a consistent boost across settings compared to WS -7.1 F1 point higher average quality and up to $13.8 \mathrm{~F} 1$ points on the SWDE University setting. Additionally handling abstensions leads to a $1.9 \mathrm{~F} 1$ point increase in average quality over WS with filtering, with up to $7.8 \mathrm{~F} 1$ points on the FDA setting. Qualitatively, accounting for abstensions is helpful when attributes are expressed in diverse ways across documents, which is not applicable to all settings such as Enron. These results highlight the importance of EVAPORATE-CODE+'s aggregation approach for the system's overall reliability. Without Algorithm 1, quality does not improve over EVAPORATE-DIRECT.

${ }^{2}$ When we replace text-davinci-003 with text-curie-001, a smaller, but cheaper LLM, an average of 5.1 gold attributes are missing per document and an average of 30.3 attributes are identified, but not explicitly mentioned in the documents.

\begin{center}
\begin{tabular}{lcc|cc}
\hline
Source & MV & WS & $\begin{array}{c}\text { WS } \\ \text { Filter }\end{array}$ & $\begin{array}{c}\text { WS } \\ \text { Abstain + Filter }\end{array}$ \\
\hline
FDA & 52.9 & 51.1 & 55.0 & 62.8 \\
Enron Emails & 81.4 & 82.7 & 86.9 & 86.9 \\
Wiki NBA & 59.5 & 64.9 & 68.4 & 68.2 \\
SWDE Movie & 44.3 & 46.3 & 56.6 & 56.8 \\
SWDE University & 42.7 & 43.5 & 57.3 & 59.0 \\
\hline
Average & $\mathbf{5 6 . 2}$ & $\mathbf{5 7 . 7}$ & $\mathbf{6 4 . 8}$ & $\mathbf{6 6 . 7}$ \\
\hline
\end{tabular}
\end{center}

Table 4: Quality under alternate approaches of aggregating the synthesized functions. The two key baselines in prior work are (left columns): Majority Vote (MV) and Weak Supervision (WS). We evaluate the components of Algorithm 1 (right columns): "Abstain" indicates we account for abstensions and "Filter" indicates we filter low quality functions.

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Model} & \multicolumn{5}{|c|}{EVAPORATE-DIRECT} & \multicolumn{5}{|c|}{EVAPORATE-CODE +} & \multirow{2}{*}{$\begin{array}{c}\text { SCHEMA ID } \\
F 1 @ k\end{array}$} \\
\hline
 & $F D A$ & Wiki & Movie & University & Enron & $F D A$ & Wiki & Movie & University & Enron &  \\
\hline
OpenAI GPT-4 [49] & 59.2 & 40.5 & 35.1 & 56.1 & 92.7 & 57.5 & 61.4 & 54.9 & 57.2 & 85.5 & 67.3 \\
\hline
Anthropic Claude-V1 [4] & 45.1 & 20.6 & 27.5 & 44.3 & 88.1 & 44.4 & 33.5 & 38.7 & 30.4 & 84.7 & 69.0 \\
\hline
Jurassic Jumbo-2-Instruct [41] & 25.9 & 0.0 & 13.3 & 29.2 & 90.3 & 1.2 & 0.0 & 20.6 & 18.6 & 85.7 & 62.3 \\
\hline
\end{tabular}
\end{center}

Table 5: OpenIE (Pair F1) results evaluating EvAPORATE using alternate LMs from three model providers. For cost reasons, we apply EVAPORATE-DIRECT to samples of 10 documents each. For fair comparison, we report the score of EVAPORATE-CODE+ on the same sample instead of the full set of documents. $k$ is the number of gold attributes for the setting.

\subsubsection{Understanding the Tradeoff Space across Varied Language Models}
The are an increasing number of LLMs being made available. These models are trained by various providers each using distinct protocols [40]. To understand whether the tradeoffs we identified hold for different LLMs, we evaluate EVAPORATE using three additional LLMs from three different providers: (1) GPT-4 [49], (2) Anthropic Claude-V1 [4], and (3) Jurassic Jumbo-2-Instruct [41]. Results are summarized in Table 5.

Overall results. The quality with gpt-4 is comparable to that obtained using text-davinci-003. Both the EVAPORATE-DIRECT and EVAPORATE-CODE+ quality decrease with claude and jumbo, consistent with the results of large-scale benchmarking efforts [40], however the relative quality of the two implementations are similar to Table 3. Both appear to remain competitive in quality and the quality of the approaches appear to increase together.

We find the precision of EVAPORATE-CODE+ remains high across models. Algorithm 1 helps EVAPORATE filter the low quality functions and if this eliminates all the candidate functions, the attribute is excluded from the output table. We find that when an attribute is included in the output, it has high precision, consistent with Table 1 where the precision with text-davinci-003 is almost 20 points higher than the recall. The average precision scores corresponding to EVAPORATE-CODE+ in Table 5 are 70.9 (gpt-4), 67.6 (claude), and 50.9 (jumbo) using EVAPORATE-CODE+ and in contrast are 61.9 ( gpt-4), 55.1 (claude), and 49.9 (jumbo) using EVAPORATE-DIRECT, emphasizing a precision-recall tradeoff between approaches.

Understanding the errors. Overall, EVAPORATE relies on versatile reasoning capabilities (i.e. to identify the schema and extract attribute values directly from noisy provided context, and the ability to synthesize code) and excitingly, the results validate that these capabilities co-exist within multiple model families. We investigate which of the required reasoning capabilities contributes to lower quality in comparison to text-davinci-003. We find that the schema synthesis step plays a small role. Considering the top ranked schema attributes according to EVAPORATE, we measure the average F1@ $k$ between the predicted and gold sets of attributes, where $k$ is the number of gold attributes per setting. The average F1@ $k$ for text-davinci-003 is 71.9, and the right-hand column of Table 5 shows the alternate models perform comparably.

We find the two main sources of errors are (1) the inability to generate a function for particular attributes, and (2) occasionally, low quality direct extractions (e.g., claude may generate "I'm not sure, please give me more information." in a ChatBot style, when prompted to extract an attribute value). The models we evaluate are optimized for language and personal assistant (ChatBot) applications [37]. Our proof of concept with gpt-4 suggests these objectives are not orthogonal to code synthesis and the quality gaps could close in future model versions.

\section{Discussion}
The goal of this study was to evaluate a simple, prototype system that uses LLMs to generate structured views from unstructured documents. We explored two fundamentally different implementation strategies and next, highlight opportunities for future work in the space.

Extending to additional tasks. Our findings demonstrate the promise of function synthesis as a way to mitigate cost when using LLMs. We study the problem of materializing a structured view of an unstructured dataset, but this insight may be applicable in a broader suite of data wrangling tasks. Many data wrangling tasks are high throughput applications, for which LLMs are not optimized. Future work should explore whether code synthesis may enable general low cost solutions.

Characterizing function cost. We dichotomized the direct extraction and code synthesis implementations. However, the lines between these approach may blur going forward. After all, the LLM could generate functions that invoke other models - for instance, the LLM may generate functions that call the NLTK, HUGGINGFACE, or even the OPENAI APIs. This naturally raises the question of how to characterize the cost of the generated functions, rather than assuming they are inexpensive.

Improving function quality. Finally, future work should consider iterative approaches to function generation. Concretely, when a generated function fails to compile or achieves low scores compared to the high quality LLM, we may be able to provide the compilation errors and/or high quality LLM responses in a prompt that encourages the LLM to generate an improved function. For instance, we may use Algorithm 1 to score the quality of small LLMs for performing the extractions.

Potential downstream uses. A structured representation compiled over heterogeneous documents is useful in fulfilling downstream queries over the data. In the absence of a structured representation, querying an un/semistructured data lake can be costly. For instance, recent works [16, 35] utilize LLMs to produce answers to natural language queries over heterogeneous documents with a retrieve-and-process pipeline: they (1) retrieve relevant data sources and (2) apply the query (or sub-queries) to each of the sources respectively. Since steps (1) and (2) are done at run-time for each query, the method is costly in terms of the required inference calls. Instead, an eagerly prepared structured representation enables support for direct SQL workloads over the attributes outputted by EVAPORATE.

Further, in cases where all the information required for a user's question is not included in the output table, EVAPORATE's output may still help improve the cost and quality of the traditional retrieve-and-process pipeline used in those prior works [15, 16, 35]. For instance, consider the question "How many times did the Enron and Disney CEOs meet in 1999?". Even though Enron-Disney CEO meeting may not be an attribute in the EVAPORATE output, we could still filter the 500k emails in the document-set to those timestamped with "1999", as the date attribute is included in the output table. The ability to filter based on EVAPORATE's output may lead to downstream improvements in retrieval cost and quality.

\section{Related Work}
Structured querying of heterogeneous data. Converting heterogeneous data to structured databases is a long standing data management problem [10, 14, 29, inter alia.]. In contrast to systems for knowledge base construction (KBC) or closed information extraction (IE) $[44,55]$, which assume there is a predefined schema and focus on populating the database according to the schema, the setup we focus on relies on OpenIE. OpenIE is the task of extracting useful facts without access to a predefined ontology (i.e. the types or categories of facts to be extracted) $[5,19]$. Given the breadth of input documents, the ability to construct a schema and populate the corresponding database on-the-fly is useful.

Existing systems for this problem introduce assumptions about the data-domain [21, 22, 34], file-format (e.g., XML files) [28], or the syntactic patterns of useful facts [10, 13, 23, 29, 38, 45, 48, 63]. For instance, in early systems, Cafarella et al. [13] focuses on facts expressed as triples (two entities with a descriptive string of their relationship in between) with hypernym "is-a" relationships between the entities. In recent years, deep learning based systems have begun to outperform the rule based systems in various settings [63]. However, the existing neural systems (1) require domain- and document-format specific training, (2) focus on reasoning over sentences, in contrast to long documents, and (3) rely on high quality linguistic tools (e.g. dependency parse, POS, NER) to help introduce structure over unstructured text $[38,63]$.

For the narrower problem of generating structured views from web data $[13,23,24]$, the current state-of-the-art approaches use (1) distant supervision to train site-specific extraction models [42] (domain specific), and (2) rely on assumptions about where in the HTML-DOM attributes and values are located (format specific) Deng et al. [21], Lockard et al. [43]. We investigate the feasibility of a domain and document-format agnostic approach.

Language models for data management. Given the recency of in-context learning, there are few works exploring the benefits for data processing. Most closely related, Chen et al. [16] presents a system for querying heterogeneous data lakes with in-context learning. The proposed approach involves processing every document with the LLM to extract values of interest. We propose an alternate approach and tradeoff space for processing data with LLMs.

In-context learning has also recently been applied to data wrangling [46] and processing SQL queries [57]. These works require a manual prompt design step, which is a bottleneck for data management systems, since documents contain a wide variety of formats, attribute-types, and topics. In contrast, in EVAPORATE a handful of prompts are used across all document settings and included in Appendix E - the prompts are not modified in any way from setting to setting.

Data programming. We build on work in data programming and weak supervision [51], a broad set of techniques for aggregating cheaply available sources of information (for instance functions based on heuristics and knowledge bases) in a principled statistical framework. WS is widely used for data management in industry. In this work, we automatically generate the functions rather than using human-designed functions. We design a system for open ended tasks in contrast to the classification tasks considered in the prior work on automated WS [7, 58], and we propose a strategy that uses the LLM to handle abstensions and filter low quality functions.

Recent work considers combining weak supervision and LLM in-context learning [3]. Our approach applies to a complementary set of problems for which processing every data point (document) with the LLM would be expensive, and requires addressing the challenges described in Section 3.3 to apply WS.

\section{Conclusion}
We propose and evaluate EVAPORATE, a system that uses LLMs to generate structured views of semi-structured data lakes. Our evaluation focuses on the tradeoff between cost, quality, and generality. We find that LLM-based systems work across document formats and domains, and therefore expose a more general interface than state-of-the-art approaches. We identify and explore a cost-quality tradeoff between processing data directly with an LLM versus synthesizing code for data processing. Finally, we propose an extension of the code synthesis implementation based off weak supervision. Our study highlights the promise of LLM-based data management systems.

\section{A Experimental Details}
We describe the metrics we use to evaluate OpenIE and ClosedIE performance of our system.

Pair F1 For OpenIE, we report Pair F1 scores. Pair F1 is the standard metric for OpenIE systems [42]. The metric constructs (subject, value, predicate). The subject is the document-filename in our setting. The predicate is the attribute and the value is the attribute value. The F1 score computes the F1 score between the sets of gold and predicted tuples. This assigns credit for exact-matches between the attribute names and values extracted by the system and the ground truth — it assigns no partial credit.

Note that because EVAPORATE first identifies a list of attributes then sequentially generates functions and extracts the values, the user can "stop" execution at any number of attributes. The stopping point determines the number of tuples included in the prediction set. This is not a property of prior systems that extract "all or no" tuples [21, 38, 42, 43, 63, inter alia.]. For fair comparison we report performance at the number of gold attributes contained in the benchmark note that this is generally not the number of attributes that maximizes the EVAPORATE's Pair F1 score.

Text F1 For ClosedIE, we report Text F1 scores. Text F1 is the standard metric for extractive tasks and we use the exact implementation released by Rajpurkar et al. [50]. The metric tokenizes the prediction and gold strings and computes a token-wise F1 score.

Recall we select the F1 at the number of gold attributes, rather than at the number that gives the highest score).

\section{B Dataset Construction}
Below we describe how each of the evaluation benchmarks is obtained. We also release the suite of benchmarks along with the system code.

\section{B.1 FDA}
For the FDA setting, we randomly sampled a dataset of 100 FDA 510(k) premarket notification submission PDFs for medical devices with substantially equivalent predicate devices since 1996 from the FDA website: https: //www . \href{http://accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm}{accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm}. We used the lightweight fitz library to convert this to text files. We asked 5 database graduate students to identify important attributes, defined as attributes useful for analysis that's present in at least a majority of documents. We collected the final set of 16 attributes as attributes agreed on by all graduate students. For these 16 attributes, we manually wrote functions to extract their value, then corrected errors by manual review. We defined the attribute value as the full content of information pertaining to that attribute, typically a value, sentence, or section.

\section{B.2 Wiki NBA}
For the Wiki NBA setting, we used the following SPARQL query over Wikidata to retrieve NBA articles. We then manually supplemented missing pages and filtered the results to only include pages about NBA players.

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-18}
\end{center}

We asked 5 database graduate students to identify important attributes, defined as attributes useful for analysis that's present in at least a majority of documents. We collected the final set of 19 attributes as the attributes agreed on by all graduate students. For these 19 attributes, we manually wrote functions to extract their value, then corrected errors by manual review. We defined the attribute value as the full content of information pertaining to that attribute, typically a value, sentence, or section. We use these as ground truth extractions in the main paper.

We noted that the resulting attributes were complex for multiple documents, so we included another set of ground truth. We asked a graduate student to write functions to parse compound attributes whose values mention multiple values (e.g. birth date and location under attribute "Born") into atomic attributes and values. We this ground truth to demonstrate an additional step of schema cleaning in Section C.3.

\section{B.3 Enron}
We download the Enron corpus from \href{http://www.cs.cmu.edu/}{http://www.cs.cmu.edu/} enron/ and apply no further processing. We generate a benchmark using all metadata in the email headers by manually writing functions.

\section{B.4 SWDE}
We download the SWDE benchmark from \href{https://www}{https://www}. \href{http://colinlockard.com/expanded_swde.html}{colinlockard.com/expanded\_swde.html}. The benchmark includes the raw HTML from several websites with no further processing and ground-truth labels for selected attributes [42]. Because all the attributes are located within the root-elements of the HTML body, excluding the information such as the HTML header, attributes described within tags (e.g. <a href='/year/2012/>', ), and so on, we extend the original benchmark to include a more diverse set of attributes. We refer to the extended benchmark as SWDE Plus.

\section{Additional Experiments and Analysis}
Here we study the effectiveness of relevant baselines, which ultimately performed poorly on our settings. We also report evaluations on additional foundation models. We focus on the Wikipedia and FDA settings for these analyses.

\section{C.1 Additional Baselines from Prior Work}
Here we study two additional baselines from prior work for OpenIE and ClosedIE respectively.

OpenIE We apply the OpenIE6 system from Kolluru et al. [38], which is a state-of-the art approach for OpenIE over unstructured (non-HTML) text. While this system is not designed for extracting structured views over heterogeneous data, we evaluate it qualitatively to understand how existing OpenIE systems perform in this setting.

\begin{enumerate}
  \item First, the system only handles well formatted sentences and struggles to extend to heterogeneous data types. It does not handle HTML documents and is difficult to apply to documents with complex formatting (like PDFs) where full sentences can be difficult to extract. Using SWDE College Prowler as an example, given the HTML input line $<$ td $>$ Student Body Size: $</$ td $><$ td class="stat"> $<$ span id="..." $6,504</$ span> $</$ td $>$, OpenIE6 misses the student body size attribute and corresponding value.

  \item Second, even when documents contain full sentences, OpenIE6 extracts an extremely large set of relations for each document and does not prioritize attributes by relevance or enforce consistent attributes across documents. This makes it difficult to use the resulting relations to understand the contents of the documents or to do analysis. Using a sample FDA 510(k) document from our corpus as an example, OpenIE6 extracts 427 relations with 184 relations with confidence larger than 0.5. While some can be informative, a significant fraction of these relations are not useful for indexing and analysis across documents. For example, "(sample carryover; occurs; the results of the acceptor assay show interference)" is an extracted relation with confidence 0.99 .

\end{enumerate}

ClosedIE We study the effectiveness of span-extractor models, which are commonly used in QA systems to extract the information that is relevant to a user-query from a provided document context [18]. Given the ground truth attributes, we evaluate the ability of these models to extract their values from the relevant paragraphs. We evaluate the DebertaV3 Large model fine-tuned on the Squad 2.0 dataset, which achieves 90.8 F1 on the Squad 2.0 dev set in Table 6. We find our EVAPORATE function generation approach (Table 1) significantly outperforms this pre-trained QA model on ClosedIE in all settings, over text and HTML documents.

\section{C.2 Validating the Quality of LLM $\mathcal{F}$}
Because our approach for scoring the generated functions relies on comparing to the extractions produced by directly processing the document with the LLM $\mathcal{F}$ (Section 3.2.2), in Table 7, we evaluate the quality of $\mathcal{F}$. In this ClosedIE task, the LLM is provided with the specific attributes to extract in the prompt. This prompt is shown in Appendix E.

\begin{center}
\begin{tabular}{lcc}
\hline
Source (Format) & \# Attributes & Closed IE F1 \\
\hline
Enron Emails (TXT) & 15 & 53.7 \\
FDA (TXT) & 17 & 56.5 \\
Wiki NBA (HTML) & 19 & 50.2 \\
SWDE Movies (HTML) & 30 & 43.5 \\
SWDE University (HTML) & 25 & 45.3 \\
\hline
\end{tabular}
\end{center}

Table 6: ClosedIE results using the DebertaV3 large model fine-tuned on the Squad2.0 dataset from HuggingFace.

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\multirow[t]{2}{*}{Source (Format)} & \multicolumn{2}{|c|}{QUALITY} & \multicolumn{2}{|c|}{$\begin{array}{c}\text { Cost / 10K } \\
\text { DoCUMENTS }\end{array}$} \\
\hline
 & \# Attributes & $F 1$ & Tokens (M) & Dollars $(\$)$ \\
\hline
Enron Emails (TXT) & 15 & 85.3 & 140 & 2,790 \\
\hline
FDA (TXT) & 16 & 78.0 & 241 & 4,816 \\
\hline
Wiki NBA (HTML) & 19 & 84.6 & 328 & 6,559 \\
\hline
SWDE Movies (HTML) & 25 & 84.4 & 359 & 7,174 \\
\hline
SWDE University (HTML) & 33 & 72.6 & 379 & 7,586 \\
\hline
Average & 21.6 & 79.9 & 289 & 5,785 \\
\hline
\end{tabular}
\end{center}

Table 7: Quality and cost achieved through prompting OpenAI's text-davinci-003 model to extract specific, pre-defined attributes.

\section{C.3 Atomic Schema Cleaning Extensions}
One further extension of schema generation is atomic schema cleaning. EVAPORATE generates a set of candidate attributes which, in some cases, are complex and can be decomposed into cleaned, atomic attributes. For example, an extracted attribute of born from the Wiki NBA setting, has the following form:  (age) . Decomposing the born attribute into three separate attributes (e.g., birth date, age and birth location) would enable users to ask queries such as - How many players in the NBA were born in Ohio? - that would otherwise be unanswerable with the existing schema. As such, decomposing the complex attributes into cleaned, atomic attributes increases the utility of the resulting schema and extractions for analysis and indexing. Prior work [46] has demonstrated that LLMs can be useful for data transformation task. Schema decomposition can be viewed as an instantiation of data transformation, suggesting that such an operation could be completed using an LLM.

We manually clean the ground truth complex attributes and values in the Wiki NBA setting and construct the ground truth atomic attribute and values. We find that after cleaning there are 35 atomic attributes for Wiki NBA, decomposed from the 19 complex attributes.

For our method, we prompt the expensive LLM (in this case text-davinci-003 from OpenAI) to decompose the complex attribute and values into a list of atomic attributes and values, for a single example of each complex attribute and value. To save computation cost, we then use the large LLM schema cleaning result from one example to prompt a smaller, less expensive LLM (in this case the text-curie-001 model from OpenAI) to extract the cleaned values for the remainder of documents. We provide the smaller, less expensive LM with the complex attribute, the cleaned attribute to extract, and a one-shot example from the expensive LLM.

To measure the performance of schema cleaning, we construct pairs of (file, value) for all files and values and compute the precision, recall, and F1 as in the OpenIE setting against the ground truth atomic attributes and values. We do not include the attribute in the relations to score, because the extracted values are generally unique and we want to avoid penalizing generated atomic attribute names that differ from the ground truth but are still correct. As a baseline before our atomic schema cleaning step, we score the ground truth complex values against the ground truth atomic values and find it achieves an F1 of 21.0, since many values are not atomic. Applying our atomic schema cleaning methodology to the ground truth complex values decomposes them into atomic attributes, qualitatively improving the usability of the attributes. The resulting predicted atomic attributes achieve an F1 of 57.5 when scored against the ground truth atomic values.

\section{Weak Supervision Details}
Objective We detail the weak supervision (WS) algorithm used for aggregating the votes across generated functions. Let $\mathcal{D}$ be our unlabeled dataset of documents from which we are extracting a particular attribute of interest. Let $y$ be a random variable representing the true attribute value. Let $\boldsymbol{\lambda}$ represent the outputs of our $m$ generated extraction functions $f \in \mathcal{E}$ on a particular document. Each $\lambda_{i} \in \boldsymbol{\lambda}$ is a function $\lambda_{i}: \mathcal{X} \rightarrow \mathcal{Y}$. Our goal is to use the vectors $\boldsymbol{\lambda}$, produced across documents $x \in \mathcal{D}$ to infer the true label $y$. Concretely, we seek, $\phi(x)$, a function that inputs $\boldsymbol{\lambda}$ and outputs a final prediction $\hat{y}$ for a document $x$.

With labeled data, i.e. where we had documents and their attribute values $\left\{\left(x_{1}, a_{1}\right), \ldots,\left(x_{n}, a_{n}\right)\right\}$, we could perform traditional supervised learn $\phi(x)$ that maps $\boldsymbol{\lambda}$ to $y$. However, in our unlabeled setting, the insight is to use the noisy estimates of $y$ produced by each of the functions to construct $\phi(x)$.

Standard WS Setup [51] WS models learn the latent variable graphical model on the distribution $\operatorname{Pr}\left(y,\left\{\lambda_{1}, \ldots, \lambda_{m}\right\}=\operatorname{Pr}(y, \boldsymbol{\lambda})\right.$. In this model, $y$ is latent, we cannot observe it. To produce $\hat{y}$, we concretely perform two steps:

\begin{itemize}
  \item Learn the label model We have access to $\boldsymbol{\lambda}$ and can use this to learn the label model $P(\boldsymbol{\lambda} \mid y)$.

  \item Inference We can then produce the estimate $\hat{y}$ by setting $\phi(x)=\operatorname{argmax}_{y} \operatorname{Pr}(y \mid \boldsymbol{\lambda}(\boldsymbol{x}))$

\end{itemize}

To learn the label model, we can parameterize $P(\boldsymbol{\lambda} \mid y)$ as follows:

$$
P(\boldsymbol{\lambda} \mid y)=\frac{1}{Z_{\theta}} \exp \left(\sum_{i=1}^{m} \theta d\left(\lambda_{i}, y\right)\right)
$$

Intuitively, when modeling our sources, we want to model how accurate a particular $\lambda_{i}$ is, when it makes a prediction. We are then going to want to weigh the predictions of the different sources proportional to their accuracies.

The $Z$ is a constant to normalize the probability distribution. The feature-vector $d$ can be broken down into:

\begin{itemize}
  \item $d_{\text {lab }}$, representing how frequently $\lambda_{i}$ provides a prediction vs. abstains across documents. This is directly observable.

  \item $d_{\text {corr }}$, which represents how frequently $\lambda_{i}$ and $\lambda_{j}$ yield the same prediction for the attribute value. This is directly observable.

  \item $d_{\mathrm{acc}}$, representing the accuracy of $\lambda_{i}$, or how frequently $\lambda_{i}$ agrees with $y$. Note that the accuracy is measured across documents for which the function provides a prediction and does not abstain. This is not directly observable.

\end{itemize}

$\theta$ are the parameters we aim to learn to combine the inputs in the feature vector. To learn this without access to $y$, we can minimize the negative log marginal likelihood given the observed $\lambda_{i}$ outputs and solve with SGD, or use a closed-form solution [26].

\section{E Prompts}
Here we include all prompts used in the paper.

\section{E.1 End-to-end prompt}
The prompt that instructs the model to produce all useful attribute-value pairs mentioned in a "chunk" of document text is as follows. This prompt is used both as the end-to-end prompt that is used to process all documents in EVAPORATEDirect (Section 3.1) and in EVAPORATE-CODE for performing Schema Identification (Section 3.2). The same prompt is applied to multiple chunks across multiple documents across our 16 evaluation settings, and contains generic in-context examples.

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-22}
\end{center}

\section{E.2 Attribute Extraction Prompt}
The following prompt is used to directly extract a provided "attribute" value from the provided "chunk" of document text. This prompt is used as EVAPORATE's prompt for collecting the high-quality $\mathbf{L} \mathbf{L} \mathbf{M} \mathcal{F}$ 's "labels" on $\mathcal{D}_{\text {eval }}$, against which the generated functions $f \in \mathcal{E}$ are scored (Section 3.2). The same prompt is applied to multiple chunks across multiple documents across our 16 evaluation settings, and contains generic data-lake agnostic in-context examples.

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-23(1)}
\end{center}

\section{E.3 Function Generation Prompts}
We use two generic prompts for function-generation as follows. These prompts correspond to $P_{A}$ and $P_{B}$ described in the Section 3.2.2 micro-experiments. The prompts accept "chunks" of document text that need to be parsed, the "attribute" of interest, and the cleaned attribute name "function field" (i.e. because attributes with "”", “-", etc. will not compile).

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-23}
\end{center}

The second prompt is:

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-24}
\end{center}

\section{E.4 Unsupervised Schema Validation Prompts}
The following prompt is used to determine the validity of an attribute identified and extracted during EVAPORATE's OpenIE execution. We apply the prompt to a small sample of (e.g. 5) values extracted from the documents, in turn, for a particular "attribute". If the LLM outputs "No" for all values in the sample, the entire attribute is discarded from EVAPORATE s outputted structured view.

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-25}
\end{center}

\section{E.5 Atomic Schema Cleaning Prompts}
The following prompts are used for atomic schema cleaning in Section C.3. This prompt is used for the expensive LLM to decompose complex attributes and values into atomic attributes and values.

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-25(1)}
\end{center}

The following prompt is used to prompt the smaller, inexpensive LM to extract the cleaned value from the complex value given the cleaned attribute and a one-shot demonstration from the expensive LLM for atomic schema cleaning in Section C.3.

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_17_bf24dbb59d8f7501c43fg-26}
\end{center}

\section{F System Input and Output Diagrams}
In Figures 5, 6, and 7, we include sample inputs and outputs for EVAPORATE.
\includegraphics[max width=\textwidth, center]{2023_08_17_bf24dbb59d8f7501c43fg-26(1)}

Figure 5: Diagram depicting EVAPORATE input and sample output on the Wikipedia NBA Players (HTML) setting.


\end{document}
